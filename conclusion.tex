
Under the centre theme of securing SDN-enabled large-scale network using high-fidelity and scalable testing system, we conduct our research work in three streams.

First, we address one of the key issues in Linux-container-based network emulation (LCNE).
Even though LCNE combines many desired features of software simulation and physical testbeds,
it uses the system clock across all the containers even if a container is not being scheduled to run.
This leads to the issue of both performance and temporal fidelity, especially with high workloads.
Virtual time sheds the light on this issue by precisely scaling the time of interactions between containers and physical devices.
We develope a lightweight Linux-container-based virtual time system and integrate it to Mininet.
Except for enhancing Minint's fidelity and scalability,
our virtual time system also plays a key role for synchronizing clocks in hybrid simulation and emulation testbed.

Second, we rethink how to simulate SDN network by taking advantage of the centralized paradigm.
Following this idea, we present a model abstraction technique that effectively transforms
the network devices in an SDN-based network to one virtualized switch model.
While significantly reducing the model execution time and enabling the real-time simulation capability,
our abstracted model also preserves the end-to-end forwarding behavior of the original network.

Third, motivated by the recent advancement and success of deep neural networks,
we study the feasibility of deep learning technologies for enhancing the essential network-architecture-agnostic security building-block.
We construct the detection engine with multiple advanced deep learning models and compare their performance.

Fourth, we take a step further from the comparative study.
Our study shows that existing machine learning-based host instruction detection methods rely on handcrafted features extracted from raw binary files or disassembled code.
The diversity of such features created has made it hard to build generic malware classification systems that
work effectively across different operational environments.
To strike a balance between generality and performance,
we explore new deep learning techniques to effectively yet efficiently classify malware programs represented as their control flow graphs.
The resultant innovative system uses deep graph convolutional neural network to
embed structural information inherent in control flow graphs for effective yet efficient malware classification.

%\section{Future Work}
%Malware detection with executable is promising
%\cite{GatedConvNet, ACFG4BugSearch, GraphEmbedding, MalConvNvidia}.
%When applying the machine learning algorithms or design customized neural networks, the following
%should be taken into consideration as guidance:
%\begin{enumerate}
%\item The informative PE header of a particular Windows OS, is well-defined and also complicated.
        %Minimal domain knowledge should be relied on since they are fragile;
        %the author of the malware may intentionally leverage these rules to hide key part of the malicious code.
%\item There are high amount of positional variation presented in the executable files.
    %Since PE header 
%\end{enumerate}

%\begin{algorithm}[h]
%\DontPrintSemicolon
    %\KwIn
    %{
        %$D = $ binary code dataset. \newline
        %$bc = $ a particular binary code file in $D$. \newline
        %$g \equiv (V, E, x) = ACFG(bc) $ the ACFG of a particular binary code file. \newline
        %$\mu_v = $ feature vector of a particular node in ACFG. \newline
        %$nn \equiv $ neural network with parameters $W_1, P_1, P_2, W_2$.
    %}
    %\SetKwProg{Fn}{Function}{}{\KwRet}
    %\SetKwFunction{ACFG}{acfg\_bytecode}
    %\SetKwFunction{EmbeddingACFG}{embedding\_acfg}
    %\SetKwFunction{GetBatchEmbedding}{get\_batch\_embedding}
    %\SetKwFunction{Train}{train}
    %\Fn{\EmbeddingACFG{$g$, $nn$}} {
        %Algorithm 1 in paper\;
        %But return the set of vertex embeddings $\mu_g = \{\mu_v| \forall v \in V\}$\;
    %}
    %\Fn{\GetBatchEmbedding{$nn$}} {
        %$batch\_ACFG= $ sample $batch\_size$ binary code files from $D\_ACFG$\;
        %Pair $g \in batch\_ACFG$ with 1 positive and 1 negative ACFG to obtain
        %$batch\_pairs = {(g_i, g_j, 1), (g_i, g_j', -1) | g_i \in batch\_ACFG}$\;
        %\ForEach (// Embed $batch\_pairs$ to $batch\_embeddings$) {$(g_i, g_j, label) \in batch\_pairs$} 
        %{
            %$\mu_{g_i}$ = \EmbeddingACFG{$g_i$, $nn$}\;
            %$\mu_{g_j}$ = \EmbeddingACFG{$g_j$, $nn$}\;
            %Append $(\mu_{g_i}, \mu_{g_j}, label)$ to $batch\_embeddings$\;
        %}
        %\KwRet{$batch\_embeddings$}
    %}
    %\Fn{\Train{$D$}} {
        %Initialize parameters in $nn$\;
        %Convert all byte code files: $D\_{ACFG} = \{ g | g = $ \ACFG{$bc$} $\forall bc \in D\}$\;
        %\For (// Train $E$ epochs) {epoch = 1 to 100} {
            %\For (// Train with batch data) {i = 1 to $\frac{||D||}{batch\_size}$} {
                %$batch\_embeddings = $ \GetBatchEmbedding{$nn$}\;
                %$nn$.fit($batch\_embeddings$)\;
            %}
        %}
    %}
%\caption{End-to-End Traning Algorithm with Embedding}
%\end{algorithm}
